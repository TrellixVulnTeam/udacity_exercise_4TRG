{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see https://github.com/hankcs/udacity-deep-learning/blob/master/6_lstm.py#L627"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see https://github.com/ahangchen/GDLnotes/blob/master/src/rnn/embed_bigram_lstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists adv', 'on from the nation', 'significant than i', 'ain drugs confusio', 'ate of the origina', 't or at least not ', 'he first daily col', 'rdoo ricky ricardo']\n",
      "['dvocate social rel', 'onal media and fro', ' in jersey and gue', 'ion inability to o', 'nal document fax m', 't parliament s opp', 'ollege newspaper i', 'do this classic in']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "bigram_vocabulary_size =vocabulary_size * vocabulary_size\n",
    "class BigramBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size_in_chars = len(text)\n",
    "    self._text_size = self._text_size_in_chars//2\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "   ## \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=self._batch_size, dtype=np.int)\n",
    "    for b in range(self._batch_size):\n",
    "      #batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      \n",
    "        char_idx = self._cursor[b] *2\n",
    "        ch1 = char2id(self._text[char_idx])\n",
    "        if self._text_size_in_chars -1 == char_idx:\n",
    "            ch2 = 0\n",
    "        else:\n",
    "            ch2 =char2id(self._text[char_idx +1])\n",
    "        batch[b] = ch1 * vocabulary_size + ch2\n",
    "                      \n",
    "        self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def bi2str(encoding):\n",
    "    return id2char(encoding // vocabulary_size)+ id2char(encoding % vocabulary_size)\n",
    "      \n",
    "def bigrams(encodings):\n",
    "    return [bi2str(e) for e in encodings]\n",
    "    \n",
    "def bibatches2string(batches):\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s,bigrams(b))]\n",
    "    return s\n",
    "    \n",
    "bi_onehot = np.zeros((bigram_vocabulary_size,bigram_vocabulary_size))\n",
    "np.fill_diagonal(bi_onehot, 1)\n",
    "\n",
    "def bi_one_hot(encodings):\n",
    "    return [bi_onehot[e] for e in encodings]\n",
    "\n",
    "train_batches = BigramBatchGenerator(train_text,8,8)\n",
    "valid_batches = BigramBatchGenerator(valid_text,1,1)\n",
    "print(bibatches2string(train_batches.next()))\n",
    "print(bibatches2string(train_batches.next()))\n",
    "print(bibatches2string(valid_batches.next()))\n",
    "print(bibatches2string(valid_batches.next()))\n",
    "\n",
    "\n",
    "#def characters(probabilities):\n",
    "#  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "#  characters back into its (most likely) character representation.\"\"\"\n",
    "#  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "#def batches2string(batches):\n",
    "#  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "#  representation.\"\"\"\n",
    "#  s = [''] * batches[0].shape[0]\n",
    "#  for b in batches:\n",
    "#    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "#  return s\n",
    "\n",
    "#train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "#valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "#print(batches2string(train_batches.next()))\n",
    "#print(batches2string(train_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction,  size=vocabulary_size):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def one_hot_voc(prediction, size=vocabulary_size):\n",
    "    p = np.zeros(shape=[1, size], dtype=np.float)\n",
    "    p[0, prediction[0]] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution(size=vocabulary_size):\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 512\n",
    "embedding_size = 128\n",
    "#keep_prob = 0.6\n",
    "batch_size=32\n",
    "num_unrollings=10\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([bigram_vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "  #think a lot of time then glance at https://github.com/magicly/udacity-ud730/blob/master/6_lstm.ipynb thanks\n",
    "  ifcox = tf.concat([ix,fx,cx,ox],1)\n",
    "  ifcom = tf.concat([im,fm,cm,om],1)\n",
    "  ifcob = tf.concat([ib,fb,cb,ob],1)\n",
    " # print(ifcox.shape)\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_vocabulary_size]))\n",
    "    # one hot encoding for labels in\n",
    "  np_one_hot = np.zeros((bigram_vocabulary_size, bigram_vocabulary_size))\n",
    "  np.fill_diagonal(np_one_hot, 1)\n",
    "  bigram_one_hot = tf.constant(np.reshape(np_one_hot, -1), dtype=tf.float32,\n",
    "                                 shape=[bigram_vocabulary_size, bigram_vocabulary_size])\n",
    "  keep_prob = tf.placeholder(tf.float32)  \n",
    "  \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "   # print(i.shape)\n",
    "    #embed = tf.nn.embedding_lookup(embeddings, i)  no need here\n",
    "    #print(embed.shape)\\\n",
    "    i = tf.nn.dropout(i, keep_prob)\n",
    "    ifco_input = tf.matmul(i,ifcox) + tf.matmul(o,ifcom) +ifcob\n",
    "    input_ifco,forget_ifco,update_ifco,output_ifco = tf.split(ifco_input,4,1)\n",
    "    #print(input_ifco.shape)\n",
    "    input_gate = tf.sigmoid(input_ifco)\n",
    "    \n",
    "    forget_gate = tf.sigmoid(forget_ifco)\n",
    "    update = update_ifco\n",
    "    output_gate = tf.sigmoid(output_ifco)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #print((output_gate * tf.tanh(state)).shape)\n",
    "    output=tf.nn.dropout(output_gate * tf.tanh(state), keep_prob)\n",
    "    return output, state\n",
    "  \n",
    "  # np_embeds = np.zeros((bigram_vocabulary_size, bigram_vocabulary_size))  \n",
    " # bigramonehot = tf.constant(np.reshape(np_embeds, -1), dtype=tf.float32, shape=[bigram_vocabulary_size, bigram_vocabulary_size],\n",
    "                                   #name='bigramonehot')  \n",
    "  # Input data.\n",
    "  tf_train_data = tf.placeholder(tf.int32, shape=[num_unrollings + 1, batch_size])\n",
    "  train_data = list()\n",
    "  #for i in range(num_unrollings + 1):\n",
    "  #  train_data.append(\n",
    "    #  tf.placeholder(tf.int32, shape=[batch_size,vocabulary_size]))# shape no change ,i spend a lot of time to think in here an there!\n",
    "  for i in tf.split(value=tf_train_data, num_or_size_splits=num_unrollings + 1, axis=0):\n",
    "        train_data.append(tf.squeeze(i))\n",
    "  \n",
    "  train_labels = list()\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  for l in train_data[1:]:\n",
    "    train_labels.append(tf.gather(bigram_one_hot, l))  # change to fit bigram\n",
    "  \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "# python loop used: tensorflow does not support sequential operations yet\n",
    "  for i in train_inputs:\n",
    "    # embed input bigrams -> [batch_size, embedding_size]      \n",
    " \n",
    "   #tf.argmax(i,axis=1))#  choose the index of the max value in every row\n",
    "    output, state = lstm_cell(tf.nn.embedding_lookup(embeddings,i), output, state)\n",
    "    outputs.append(output)\n",
    "  #  print(output.shape)\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(values=train_labels, axis=0), logits=logits)) #no change !!\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "   # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(embeddings,sample_input) #tf.agrmax(sample_input,axis=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.786360 learning rate: 10.000000\n",
      "Minibatch perplexity: 885.68\n",
      "================================================================================\n",
      "729\n",
      "gfe kjthe zbcypke aiale d mxnwe bqolwce kjdmjve qle nycye cnrje oskmghkbe tmtyvbogpfe soace tmcqetcs\n",
      "729\n",
      "zqqze hxoxpftae xef e sut  swye yjjse ciyje mnqgyuehfce awdie kye qfkje qgsde cze kpshe txnnvse yce \n",
      "729\n",
      "pienvne zqsse pnpte pukse ovpajxace dae kzyhgvfnnpdbqfiue dlwpe bdofe  rwme pevte rneue lre qte khcq\n",
      "729\n",
      "fle jymde qde lie wilh rwoe nde mle mpuce vxjle gthpgzber bwfme uawoiue  lqpe zde lttcsue irbsqopze \n",
      "729\n",
      "fpxchpe ykdee jpzmcmhi kjpe djcne misse gfe qkgovkfovplde gpide lce cnwte wkensyany vewexbfyxee asyl\n",
      "================================================================================\n",
      "Validation set perplexity: 8064.18\n",
      "2.1098215579986572\n",
      "Average loss at step 100: 6.389109 learning rate: 10.000000\n",
      "Minibatch perplexity: 171.89\n",
      "Validation set perplexity: 155.14\n",
      "5.243572473526001\n",
      "Average loss at step 200: 4.846695 learning rate: 10.000000\n",
      "Minibatch perplexity: 97.28\n",
      "Validation set perplexity: 98.15\n",
      "8.339318752288818\n",
      "Average loss at step 300: 4.505004 learning rate: 10.000000\n",
      "Minibatch perplexity: 67.37\n",
      "Validation set perplexity: 75.98\n",
      "11.441407680511475\n",
      "Average loss at step 400: 4.300312 learning rate: 10.000000\n",
      "Minibatch perplexity: 96.62\n",
      "Validation set perplexity: 61.24\n",
      "14.466525077819824\n",
      "Average loss at step 500: 4.242958 learning rate: 10.000000\n",
      "Minibatch perplexity: 69.53\n",
      "Validation set perplexity: 54.49\n",
      "17.549877405166626\n",
      "Average loss at step 600: 4.058296 learning rate: 10.000000\n",
      "Minibatch perplexity: 58.20\n",
      "Validation set perplexity: 47.30\n",
      "20.653425693511963\n",
      "Average loss at step 700: 4.035075 learning rate: 10.000000\n",
      "Minibatch perplexity: 56.68\n",
      "Validation set perplexity: 45.17\n",
      "23.798102617263794\n",
      "Average loss at step 800: 3.938062 learning rate: 10.000000\n",
      "Minibatch perplexity: 56.54\n",
      "Validation set perplexity: 38.68\n",
      "26.941208600997925\n",
      "Average loss at step 900: 3.840743 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.97\n",
      "Validation set perplexity: 37.70\n",
      "30.01161503791809\n",
      "Average loss at step 1000: 3.804780 learning rate: 10.000000\n",
      "Minibatch perplexity: 50.61\n",
      "================================================================================\n",
      "729\n",
      "gcoafourld latia also histing tor in two johporticle of u dsonr the durid boppore to the perish dhou\n",
      "729\n",
      "wk its wor at one italso sined was the at the land whichincempreifing bet nors docshpzent eight for \n",
      "729\n",
      "eorm of the ere machiom one the ning a classin at caden usts colmanyy its alsomeractory stinage gung\n",
      "729\n",
      "jbred duat as was lold and hadt sin is and backs seuw physg one no slhave havently wcomper to one wa\n",
      "729\n",
      "wuedt by the s time nine nine commbaldd a reey recutynre usuecution the nine of and the defour five \n",
      "================================================================================\n",
      "Validation set perplexity: 36.69\n",
      "33.313982248306274\n",
      "Average loss at step 1100: 3.824351 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.89\n",
      "Validation set perplexity: 34.16\n",
      "36.344351291656494\n",
      "Average loss at step 1200: 3.813130 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.12\n",
      "Validation set perplexity: 29.67\n",
      "39.39874219894409\n",
      "Average loss at step 1300: 3.851193 learning rate: 10.000000\n",
      "Minibatch perplexity: 57.71\n",
      "Validation set perplexity: 28.26\n",
      "42.547903299331665\n",
      "Average loss at step 1400: 3.817075 learning rate: 10.000000\n",
      "Minibatch perplexity: 43.28\n",
      "Validation set perplexity: 27.76\n",
      "45.61045718193054\n",
      "Average loss at step 1500: 3.785197 learning rate: 10.000000\n",
      "Minibatch perplexity: 49.48\n",
      "Validation set perplexity: 27.36\n",
      "48.77628517150879\n",
      "Average loss at step 1600: 3.765370 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.06\n",
      "Validation set perplexity: 25.57\n",
      "51.898364305496216\n",
      "Average loss at step 1700: 3.736610 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.51\n",
      "Validation set perplexity: 25.98\n",
      "55.157750606536865\n",
      "Average loss at step 1800: 3.751378 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.36\n",
      "Validation set perplexity: 27.16\n",
      "58.369123697280884\n",
      "Average loss at step 1900: 3.714467 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.32\n",
      "Validation set perplexity: 26.23\n",
      "61.59505295753479\n",
      "Average loss at step 2000: 3.677116 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.81\n",
      "================================================================================\n",
      "729\n",
      "qovie b one be roe the and is and by proviadabums for using include seven all japaerdic one before o\n",
      "729\n",
      "cepted and with thand to nine secontoince certh pw lature b one zero sixssed bishattoverr nine the m\n",
      "729\n",
      "b one preswo zero zero zero one by leason germanian of morge chachs up diston encele span saversian \n",
      "729\n",
      " genture frorgain angikaer eng of cousticaded coecauidrm eliformations tect bruthe in a nepancide ra\n",
      "729\n",
      "wder or brown themicician of rue chare on may nak for is rirnramqlations which sant of as cank enced\n",
      "================================================================================\n",
      "Validation set perplexity: 25.54\n",
      "64.9681601524353\n",
      "Average loss at step 2100: 3.661408 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.70\n",
      "Validation set perplexity: 24.70\n",
      "68.05206871032715\n",
      "Average loss at step 2200: 3.635972 learning rate: 10.000000\n",
      "Minibatch perplexity: 49.32\n",
      "Validation set perplexity: 24.65\n",
      "71.10889840126038\n",
      "Average loss at step 2300: 3.602177 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.84\n",
      "Validation set perplexity: 24.86\n",
      "74.15025043487549\n",
      "Average loss at step 2400: 3.660575 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.95\n",
      "Validation set perplexity: 24.54\n",
      "77.27465867996216\n",
      "Average loss at step 2500: 3.631226 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.62\n",
      "Validation set perplexity: 25.41\n",
      "80.3070182800293\n",
      "Average loss at step 2600: 3.645290 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.58\n",
      "Validation set perplexity: 23.94\n",
      "83.33597993850708\n",
      "Average loss at step 2700: 3.590577 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.66\n",
      "Validation set perplexity: 23.77\n",
      "86.4358057975769\n",
      "Average loss at step 2800: 3.544713 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.54\n",
      "Validation set perplexity: 24.27\n",
      "89.46468114852905\n",
      "Average loss at step 2900: 3.580740 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.80\n",
      "Validation set perplexity: 24.67\n",
      "92.53869152069092\n",
      "Average loss at step 3000: 3.536914 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "729\n",
      "ber seping nine three links in the haria and leam kingssipts of the mallisteric is nat the have with\n",
      "729\n",
      "txation ature the ea traes mussits in the mansred an entries to cookenmagh moods of the kinda is may\n",
      "729\n",
      "nfluenictions of the elish the of s is of the p the also legiol then arm rs expopu occur and natemy \n",
      "729\n",
      "hrecolition partcarwher two five zero zero vexqpdshen theory the long leasuny the suppantic ney rela\n",
      "729\n",
      "dxtment zero zero one five three one nine nine four pmarth american viceii in of eng gualim gain mno\n",
      "================================================================================\n",
      "Validation set perplexity: 23.66\n",
      "95.88164830207825\n",
      "Average loss at step 3100: 3.508839 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.55\n",
      "Validation set perplexity: 23.32\n",
      "98.97672414779663\n",
      "Average loss at step 3200: 3.568201 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.37\n",
      "Validation set perplexity: 23.99\n",
      "102.18070530891418\n",
      "Average loss at step 3300: 3.587987 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.35\n",
      "Validation set perplexity: 23.68\n",
      "105.20006895065308\n",
      "Average loss at step 3400: 3.558625 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.41\n",
      "Validation set perplexity: 23.88\n",
      "108.20981693267822\n",
      "Average loss at step 3500: 3.503490 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.57\n",
      "Validation set perplexity: 22.76\n",
      "111.20720982551575\n",
      "Average loss at step 3600: 3.467509 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.95\n",
      "Validation set perplexity: 23.54\n",
      "114.28760981559753\n",
      "Average loss at step 3700: 3.452694 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.60\n",
      "Validation set perplexity: 22.96\n",
      "117.30201768875122\n",
      "Average loss at step 3800: 3.415190 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.04\n",
      "Validation set perplexity: 21.73\n",
      "120.31838059425354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 3900: 3.434409 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.54\n",
      "Validation set perplexity: 22.82\n",
      "123.4099748134613\n",
      "Average loss at step 4000: 3.517139 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.25\n",
      "================================================================================\n",
      "729\n",
      "kritions by bbarger flortury the a cany de singeshin addered by american state modspladels are mah m\n",
      "729\n",
      "zcpolatical king chaent noweder hnasicanglosue f chass to valle neyer in haurr zeg by labion h ibrea\n",
      "729\n",
      "fctism on thatland uned parate stripft hels one five zero zero george stato lack is in hember painte\n",
      "729\n",
      "lstury language chark spainsto air quise is mariar vgarcined a an numbernoted fiskial work instant t\n",
      "729\n",
      "nhy aeayile have scils stated in stennaven b one eight nine three one six jabil in massan disording \n",
      "================================================================================\n",
      "Validation set perplexity: 22.31\n",
      "126.71667528152466\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "summary_frequency = 100\n",
    "from time import time\n",
    "start = time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "  valid_batches = BigramBatchGenerator(valid_text, 1, 1)  \n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "  #  feed_dict = dict()\n",
    " #   for i in range(num_unrollings + 1):\n",
    "#    feed_dict[train_data[i]] = batches[i]\n",
    " #   _, l, predictions, lr = session.run(\n",
    " #     [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate],\n",
    "                                            feed_dict={tf_train_data: batches, keep_prob: 0.6})\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = list(batches)[1:]\n",
    "      labels = np.concatenate([bi_one_hot(l) for l in labels])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          print(bigram_vocabulary_size)\n",
    "          feed = np.argmax(sample(random_distribution(bigram_vocabulary_size),bigram_vocabulary_size))\n",
    "          sentence = bi2str(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(49):\n",
    "            prediction = sample_prediction.eval({sample_input: [feed], keep_prob: 1.0})\n",
    "            feed = np.argmax(sample(prediction, bigram_vocabulary_size))\n",
    "            sentence += bi2str(feed)\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1.0})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, one_hot_voc(b[1], bigram_vocabulary_size))\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "      end = time()\n",
    "      total = end -start\n",
    "      print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
