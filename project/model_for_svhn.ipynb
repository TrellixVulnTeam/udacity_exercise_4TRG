{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (229754, 32, 32, 3) (229754, 6)\n",
      "Validation set (6000, 32, 32, 3) (6000, 6)\n",
      "Test set (13068, 32, 32, 3) (13068, 6)\n"
     ]
    }
   ],
   "source": [
    "# function to load pickle files\n",
    "def load_pickle_file(pickle_file, dataset_str, labels_str):\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        dataset = save[dataset_str]\n",
    "        labels = save[labels_str]\n",
    "        del save\n",
    "    return dataset, labels\n",
    "\n",
    "pkl_train = 'svhn_my_all_color_train.pickle'\n",
    "pkl_test = 'svhn_my_all_color_test.pickle'\n",
    "pkl_valid = 'svhn_my_all_color_valid.pickle'\n",
    "train_dataset, train_labels = load_pickle_file(pkl_train, 'train_dataset', 'train_labels')\n",
    "test_dataset, test_labels = load_pickle_file(pkl_test, 'test_dataset', 'test_labels')\n",
    "valid_dataset, valid_labels = load_pickle_file(pkl_valid, 'valid_dataset', 'valid_labels')\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define accuracy as performance metric to be used in this exercise\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13068, 32, 32, 3)\n",
      "(6534, 32, 32, 3)\n",
      "(6534, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.shape)\n",
    "test_dataset_0=test_dataset[0:6534,:,:,:]\n",
    "test_dataset_1=test_dataset[6534:13068,:,:,:]\n",
    "#test_dataset_2=test_dataset[9000:,:,:,:]\n",
    "print(test_dataset_0.shape)\n",
    "print(test_dataset_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  2  6 10 10 10]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD+CAYAAAAalrhRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1deb14a1358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "img_test=train_dataset[10]\n",
    "plt.imshow(img_test)\n",
    "print(train_labels[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "n_inputs = height * width\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "pool3_fmaps = conv2_fmaps\n",
    "n_fc1 = 64\n",
    "n_outputs = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph() \n",
    "with tf.name_scope(\"inputs1\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None,height, width, channels], name=\"X\")\n",
    "    #X_reshaped = tf.reshape(X, shape=[-1, ])\n",
    "    y = tf.placeholder(tf.int32, shape=[None,6], name=\"y\")\n",
    "\n",
    "conv1 = tf.layers.conv2d(X, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.layers.max_pooling2d(conv2, 2,2, padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 8 * 8])\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits1 = tf.layers.dense(fc1, n_outputs, name=\"output1\")\n",
    "    logits2 = tf.layers.dense(fc1, n_outputs, name=\"output2\")\n",
    "    logits3 = tf.layers.dense(fc1, n_outputs, name=\"output3\")\n",
    "    logits4 = tf.layers.dense(fc1, n_outputs, name=\"output4\")\n",
    "    logits5 = tf.layers.dense(fc1, n_outputs, name=\"output5\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    loss1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits1, labels=y[:,1]))\n",
    "    loss2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits2, labels=y[:,2]))\n",
    "    loss3 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits3, labels=y[:,3]))\n",
    "    loss4 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits4, labels=y[:,4]))\n",
    "    loss5 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits5, labels=y[:,5]))\n",
    "    loss = loss1+loss2+loss3+loss4+loss5\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    predictions = tf.stack([tf.nn.softmax(logits1),\\\n",
    "                                tf.nn.softmax(logits2),\\\n",
    "                                tf.nn.softmax(logits3),\\\n",
    "                                tf.nn.softmax(logits4),\\\n",
    "                                tf.nn.softmax(logits5)])\n",
    "    \n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 42.8125 Test accuracy: 43.4433333333\n",
      "0 Train accuracy: 80.9375 Test accuracy: 77.9566666667\n",
      "0 Train accuracy: 79.375 Test accuracy: 82.27\n",
      "0 Train accuracy: 85.625 Test accuracy: 84.3966666667\n",
      "0 Train accuracy: 90.625 Test accuracy: 85.3633333333\n",
      "0 Train accuracy: 88.125 Test accuracy: 86.67\n",
      "0 Train accuracy: 88.75 Test accuracy: 87.24\n",
      "0 Train accuracy: 89.0625 Test accuracy: 87.7\n",
      "1 Train accuracy: 90.3125 Test accuracy: 87.8266666667\n",
      "1 Train accuracy: 92.5 Test accuracy: 87.97\n",
      "1 Train accuracy: 88.125 Test accuracy: 88.6166666667\n",
      "1 Train accuracy: 89.0625 Test accuracy: 88.5933333333\n",
      "1 Train accuracy: 93.125 Test accuracy: 88.9866666667\n",
      "1 Train accuracy: 90.3125 Test accuracy: 89.1\n",
      "1 Train accuracy: 88.125 Test accuracy: 89.4033333333\n",
      "1 Train accuracy: 88.75 Test accuracy: 89.4333333333\n",
      "2 Train accuracy: 90.625 Test accuracy: 89.5966666667\n",
      "2 Train accuracy: 93.75 Test accuracy: 89.51\n",
      "2 Train accuracy: 88.125 Test accuracy: 89.6566666667\n",
      "2 Train accuracy: 91.25 Test accuracy: 89.5433333333\n",
      "2 Train accuracy: 93.75 Test accuracy: 89.7233333333\n",
      "2 Train accuracy: 91.5625 Test accuracy: 89.7233333333\n",
      "2 Train accuracy: 89.0625 Test accuracy: 89.9633333333\n",
      "2 Train accuracy: 90.0 Test accuracy: 90.06\n",
      "3 Train accuracy: 92.1875 Test accuracy: 90.0066666667\n",
      "3 Train accuracy: 93.75 Test accuracy: 90.1066666667\n",
      "3 Train accuracy: 88.125 Test accuracy: 90.0666666667\n",
      "3 Train accuracy: 91.25 Test accuracy: 90.0566666667\n",
      "3 Train accuracy: 94.375 Test accuracy: 90.1233333333\n",
      "3 Train accuracy: 92.1875 Test accuracy: 90.21\n",
      "3 Train accuracy: 90.625 Test accuracy: 90.31\n",
      "3 Train accuracy: 90.625 Test accuracy: 90.4633333333\n",
      "4 Train accuracy: 91.875 Test accuracy: 90.3666666667\n",
      "4 Train accuracy: 93.4375 Test accuracy: 90.3966666667\n",
      "4 Train accuracy: 88.4375 Test accuracy: 90.3533333333\n",
      "4 Train accuracy: 92.1875 Test accuracy: 90.3666666667\n",
      "4 Train accuracy: 94.6875 Test accuracy: 90.5566666667\n",
      "4 Train accuracy: 93.125 Test accuracy: 90.5633333333\n",
      "4 Train accuracy: 90.9375 Test accuracy: 90.5433333333\n",
      "4 Train accuracy: 91.25 Test accuracy: 90.7866666667\n",
      "5 Train accuracy: 90.9375 Test accuracy: 90.75\n",
      "5 Train accuracy: 93.75 Test accuracy: 90.75\n",
      "5 Train accuracy: 88.4375 Test accuracy: 90.7\n",
      "5 Train accuracy: 91.25 Test accuracy: 90.64\n",
      "5 Train accuracy: 95.0 Test accuracy: 90.83\n",
      "5 Train accuracy: 93.75 Test accuracy: 90.8166666667\n",
      "5 Train accuracy: 90.0 Test accuracy: 90.9\n",
      "5 Train accuracy: 91.25 Test accuracy: 91.04\n",
      "6 Train accuracy: 91.25 Test accuracy: 90.89\n",
      "6 Train accuracy: 94.6875 Test accuracy: 91.0433333333\n",
      "6 Train accuracy: 89.6875 Test accuracy: 90.91\n",
      "6 Train accuracy: 91.25 Test accuracy: 90.9133333333\n",
      "6 Train accuracy: 95.3125 Test accuracy: 91.11\n",
      "6 Train accuracy: 93.4375 Test accuracy: 90.93\n",
      "6 Train accuracy: 90.0 Test accuracy: 91.2466666667\n",
      "6 Train accuracy: 92.1875 Test accuracy: 91.3033333333\n",
      "7 Train accuracy: 92.5 Test accuracy: 91.1566666667\n",
      "7 Train accuracy: 95.625 Test accuracy: 91.4433333333\n",
      "7 Train accuracy: 89.6875 Test accuracy: 91.2666666667\n",
      "7 Train accuracy: 92.8125 Test accuracy: 91.4833333333\n",
      "7 Train accuracy: 95.3125 Test accuracy: 91.68\n",
      "7 Train accuracy: 93.75 Test accuracy: 91.5633333333\n",
      "7 Train accuracy: 91.875 Test accuracy: 91.8333333333\n",
      "7 Train accuracy: 93.4375 Test accuracy: 91.74\n",
      "8 Train accuracy: 93.125 Test accuracy: 92.0566666667\n",
      "8 Train accuracy: 95.625 Test accuracy: 91.9633333333\n",
      "8 Train accuracy: 90.9375 Test accuracy: 91.8\n",
      "8 Train accuracy: 94.6875 Test accuracy: 91.93\n",
      "8 Train accuracy: 96.25 Test accuracy: 91.9633333333\n",
      "8 Train accuracy: 94.375 Test accuracy: 91.83\n",
      "8 Train accuracy: 92.1875 Test accuracy: 91.9233333333\n",
      "8 Train accuracy: 93.75 Test accuracy: 91.8466666667\n",
      "9 Train accuracy: 94.0625 Test accuracy: 92.2\n",
      "9 Train accuracy: 96.25 Test accuracy: 92.1266666667\n",
      "9 Train accuracy: 90.9375 Test accuracy: 91.9333333333\n",
      "9 Train accuracy: 94.375 Test accuracy: 92.0966666667\n",
      "9 Train accuracy: 95.9375 Test accuracy: 92.0633333333\n",
      "9 Train accuracy: 94.6875 Test accuracy: 91.95\n",
      "9 Train accuracy: 92.1875 Test accuracy: 91.98\n",
      "9 Train accuracy: 94.375 Test accuracy: 91.9433333333\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(train_dataset)// batch_size):\n",
    "            offset = (iteration * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            X_batch = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            y_batch = train_labels[offset:(offset + batch_size),:]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            if iteration %500 ==0:\n",
    "                train_pre = predictions.eval(feed_dict={X:X_batch})\n",
    "                #print(type(train_pre))\n",
    "                val_pre = predictions.eval(feed_dict={X:valid_dataset})     \n",
    "                #print(type(val_pre))\n",
    "                acc_val = accuracy(val_pre, valid_labels[:,1:6])\n",
    "                acc_train = accuracy(train_pre,y_batch[:,1:6])\n",
    "                print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_val)\n",
    "\n",
    "        save_path = saver.save(sess, \".ipynb_checkpoints/2.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .ipynb_checkpoints/2.ckpt\n",
      "[[ 3]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [10]\n",
      " [10]] [ 3  1  5 10 10]\n"
     ]
    }
   ],
   "source": [
    "# test use a singe example\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,\".ipynb_checkpoints/2.ckpt\" )\n",
    "    pre = np.argmax(predictions.eval(feed_dict={X:train_dataset[999:1000]}),2)\n",
    "    print(pre,train_labels[999,1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  try to add a conv and pool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None,height, width, channels], name=\"X\")\n",
    "    #X_reshaped = tf.reshape(X, shape=[-1, ])\n",
    "    y = tf.placeholder(tf.int32, shape=[None,6], name=\"y\")\n",
    "    conv1 = tf.layers.conv2d(X, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "    pool1 = tf.layers.max_pooling2d(conv1,pool_size=2,strides=2,padding='same')\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "    pool2 = tf.layers.max_pooling2d(conv2,pool_size=2,strides=2,padding='same')\n",
    "    conv3 = tf.layers.conv2d(pool2,filters=128,kernel_size=conv2_ksize,\n",
    "                        strides=conv2_stride,padding=conv2_pad)\n",
    "    pool3 = tf.layers.max_pooling2d(conv3, 2,2, padding=\"same\")\n",
    "    pool3_flat = tf.layers.flatten(pool3)\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "    logits1 = tf.layers.dense(fc1, n_outputs, name=\"output1\")\n",
    "    logits2 = tf.layers.dense(fc1, n_outputs, name=\"output2\")\n",
    "    logits3 = tf.layers.dense(fc1, n_outputs, name=\"output3\")\n",
    "    logits4 = tf.layers.dense(fc1, n_outputs, name=\"output4\")\n",
    "    logits5 = tf.layers.dense(fc1, n_outputs, name=\"output5\")\n",
    "    loss1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits1, labels=y[:,1]))\n",
    "    loss2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits2, labels=y[:,2]))\n",
    "    loss3 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits3, labels=y[:,3]))\n",
    "    loss4 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits4, labels=y[:,4]))\n",
    "    loss5 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits5, labels=y[:,5]))\n",
    "    loss = loss1+loss2+loss3+loss4+loss5\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    predictions = tf.stack([tf.nn.softmax(logits1),\\\n",
    "                                tf.nn.softmax(logits2),\\\n",
    "                                tf.nn.softmax(logits3),\\\n",
    "                                tf.nn.softmax(logits4),\\\n",
    "                                tf.nn.softmax(logits5)])\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 39.375 Test accuracy: 36.13\n",
      "0 Train accuracy: 76.5625 Test accuracy: 77.7633333333\n",
      "0 Train accuracy: 81.25 Test accuracy: 84.1733333333\n",
      "0 Train accuracy: 88.125 Test accuracy: 87.2\n",
      "0 Train accuracy: 90.625 Test accuracy: 88.1466666667\n",
      "0 Train accuracy: 92.8125 Test accuracy: 89.6633333333\n",
      "0 Train accuracy: 91.25 Test accuracy: 89.9066666667\n",
      "0 Train accuracy: 89.375 Test accuracy: 90.6366666667\n",
      "1 Train accuracy: 93.125 Test accuracy: 91.2266666667\n",
      "1 Train accuracy: 95.0 Test accuracy: 91.4833333333\n",
      "1 Train accuracy: 89.6875 Test accuracy: 91.8066666667\n",
      "1 Train accuracy: 93.125 Test accuracy: 92.25\n",
      "1 Train accuracy: 95.625 Test accuracy: 92.2433333333\n",
      "1 Train accuracy: 95.9375 Test accuracy: 92.7\n",
      "1 Train accuracy: 91.25 Test accuracy: 92.8\n",
      "1 Train accuracy: 91.875 Test accuracy: 92.9233333333\n",
      "2 Train accuracy: 92.8125 Test accuracy: 92.9366666667\n",
      "2 Train accuracy: 95.9375 Test accuracy: 93.0733333333\n",
      "2 Train accuracy: 91.875 Test accuracy: 93.29\n",
      "2 Train accuracy: 94.0625 Test accuracy: 93.4966666667\n",
      "2 Train accuracy: 96.25 Test accuracy: 93.5433333333\n",
      "2 Train accuracy: 96.875 Test accuracy: 93.5433333333\n",
      "2 Train accuracy: 93.75 Test accuracy: 93.9033333333\n",
      "2 Train accuracy: 95.0 Test accuracy: 93.6\n",
      "3 Train accuracy: 95.3125 Test accuracy: 93.8533333333\n",
      "3 Train accuracy: 97.1875 Test accuracy: 93.9033333333\n",
      "3 Train accuracy: 91.875 Test accuracy: 93.9633333333\n",
      "3 Train accuracy: 95.9375 Test accuracy: 94.11\n",
      "3 Train accuracy: 96.875 Test accuracy: 94.0633333333\n",
      "3 Train accuracy: 97.5 Test accuracy: 94.1833333333\n",
      "3 Train accuracy: 94.6875 Test accuracy: 94.2066666667\n",
      "3 Train accuracy: 96.5625 Test accuracy: 94.13\n",
      "4 Train accuracy: 95.0 Test accuracy: 94.1033333333\n",
      "4 Train accuracy: 97.5 Test accuracy: 94.1933333333\n",
      "4 Train accuracy: 92.1875 Test accuracy: 94.1866666667\n",
      "4 Train accuracy: 96.25 Test accuracy: 94.4133333333\n",
      "4 Train accuracy: 97.8125 Test accuracy: 94.2966666667\n",
      "4 Train accuracy: 97.5 Test accuracy: 94.4233333333\n",
      "4 Train accuracy: 95.3125 Test accuracy: 94.3166666667\n",
      "4 Train accuracy: 96.25 Test accuracy: 94.37\n",
      "5 Train accuracy: 95.9375 Test accuracy: 94.1133333333\n",
      "5 Train accuracy: 96.875 Test accuracy: 94.4266666667\n",
      "5 Train accuracy: 94.0625 Test accuracy: 94.31\n",
      "5 Train accuracy: 96.5625 Test accuracy: 94.56\n",
      "5 Train accuracy: 97.1875 Test accuracy: 94.3866666667\n",
      "5 Train accuracy: 97.1875 Test accuracy: 94.5333333333\n",
      "5 Train accuracy: 94.0625 Test accuracy: 94.4733333333\n",
      "5 Train accuracy: 96.875 Test accuracy: 94.7066666667\n",
      "6 Train accuracy: 95.9375 Test accuracy: 94.3666666667\n",
      "6 Train accuracy: 97.5 Test accuracy: 94.6866666667\n",
      "6 Train accuracy: 94.375 Test accuracy: 94.3966666667\n",
      "6 Train accuracy: 96.5625 Test accuracy: 94.67\n",
      "6 Train accuracy: 96.875 Test accuracy: 94.4966666667\n",
      "6 Train accuracy: 97.8125 Test accuracy: 94.6666666667\n",
      "6 Train accuracy: 94.6875 Test accuracy: 94.4933333333\n",
      "6 Train accuracy: 97.5 Test accuracy: 94.71\n",
      "7 Train accuracy: 96.25 Test accuracy: 94.5366666667\n",
      "7 Train accuracy: 97.8125 Test accuracy: 94.7633333333\n",
      "7 Train accuracy: 95.625 Test accuracy: 94.61\n",
      "7 Train accuracy: 97.1875 Test accuracy: 94.8066666667\n",
      "7 Train accuracy: 97.1875 Test accuracy: 94.6433333333\n",
      "7 Train accuracy: 97.8125 Test accuracy: 94.7133333333\n",
      "7 Train accuracy: 95.3125 Test accuracy: 94.6033333333\n",
      "7 Train accuracy: 97.1875 Test accuracy: 94.67\n",
      "8 Train accuracy: 96.875 Test accuracy: 94.7233333333\n",
      "8 Train accuracy: 97.5 Test accuracy: 94.5533333333\n",
      "8 Train accuracy: 95.3125 Test accuracy: 94.6433333333\n",
      "8 Train accuracy: 96.875 Test accuracy: 94.8533333333\n",
      "8 Train accuracy: 97.1875 Test accuracy: 94.49\n",
      "8 Train accuracy: 98.125 Test accuracy: 94.63\n",
      "8 Train accuracy: 95.3125 Test accuracy: 94.68\n",
      "8 Train accuracy: 97.5 Test accuracy: 94.8166666667\n",
      "9 Train accuracy: 97.8125 Test accuracy: 94.83\n",
      "9 Train accuracy: 97.8125 Test accuracy: 94.7\n",
      "9 Train accuracy: 95.625 Test accuracy: 94.8666666667\n",
      "9 Train accuracy: 97.1875 Test accuracy: 94.8933333333\n",
      "9 Train accuracy: 97.8125 Test accuracy: 94.57\n",
      "9 Train accuracy: 98.125 Test accuracy: 94.6433333333\n",
      "9 Train accuracy: 95.0 Test accuracy: 94.91\n",
      "9 Train accuracy: 98.125 Test accuracy: 94.9766666667\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(train_dataset)// batch_size):\n",
    "            offset = (iteration * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            X_batch = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            y_batch = train_labels[offset:(offset + batch_size),:]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            if iteration %500 ==0:\n",
    "                train_pre = predictions.eval(feed_dict={X:X_batch})\n",
    "                #print(type(train_pre))\n",
    "                val_pre = predictions.eval(feed_dict={X:valid_dataset})     \n",
    "                #print(type(val_pre))\n",
    "                acc_val = accuracy(val_pre, valid_labels[:,1:6])\n",
    "                acc_train = accuracy(train_pre,y_batch[:,1:6])\n",
    "                print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_val)\n",
    "\n",
    "        save_path = saver.save(sess, \".ipynb_checkpoints/3.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .ipynb_checkpoints/3.ckpt\n",
      "93.6715641261\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess,\".ipynb_checkpoints/3.ckpt\" )\n",
    "    test_prediction = sess.run(predictions, feed_dict={X:test_dataset})\n",
    "    acc = accuracy(test_prediction,test_labels[:,1:6])  \n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's add layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "mygraph1 = tf.Graph()\n",
    "\n",
    "with mygraph1.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None,height, width, channels], name=\"X\")\n",
    "    #X_reshaped = tf.reshape(X, shape=[-1, ])\n",
    "    y = tf.placeholder(tf.int32, shape=[None,6], name=\"y\")\n",
    "\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    conv1 = tf.layers.conv2d(X, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "    pool1 = tf.layers.max_pooling2d(conv1,pool_size=2,strides=2,padding='same')\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "    pool2 = tf.layers.max_pooling2d(conv2,pool_size=2,strides=2,padding='same')\n",
    "    conv3 = tf.layers.conv2d(pool2,filters=128,kernel_size=conv2_ksize,\n",
    "                        strides=conv2_stride,padding=conv2_pad)\n",
    "    pool3 = tf.layers.max_pooling2d(conv3, 2,2, padding=\"same\")\n",
    "\n",
    "    conv4 = tf.layers.conv2d(pool3,filters=256,kernel_size=conv2_ksize,\n",
    "                        strides=conv2_stride,padding=conv2_pad)\n",
    "    pool4 = tf.layers.max_pooling2d(conv4, 2,2, padding=\"same\")\n",
    "    pool4_flat = tf.layers.flatten(pool4)\n",
    "    fc1 = tf.layers.dense(pool4_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "    \n",
    "    logits1 = tf.layers.dense(fc1, n_outputs, name=\"output1\")\n",
    "    logits2 = tf.layers.dense(fc1, n_outputs, name=\"output2\")\n",
    "    logits3 = tf.layers.dense(fc1, n_outputs, name=\"output3\")\n",
    "    logits4 = tf.layers.dense(fc1, n_outputs, name=\"output4\")\n",
    "    logits5 = tf.layers.dense(fc1, n_outputs, name=\"output5\")\n",
    "    loss1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits1, labels=y[:,1]))\n",
    "    loss2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits2, labels=y[:,2]))\n",
    "    loss3 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits3, labels=y[:,3]))\n",
    "    loss4 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits4, labels=y[:,4]))\n",
    "    loss5 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits5, labels=y[:,5]))\n",
    "    loss = loss1+loss2+loss3+loss4+loss5\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "    training_op = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=5e-4)\n",
    "    #training_op = optimizer.minimize(loss)\n",
    "    predictions = tf.stack([tf.nn.softmax(logits1),\\\n",
    "                                tf.nn.softmax(logits2),\\\n",
    "                                tf.nn.softmax(logits3),\\\n",
    "                                tf.nn.softmax(logits4),\\\n",
    "                                tf.nn.softmax(logits5)])\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 33.125 val accuracy: 31.33 loss 2.27038\n",
      "0 Train accuracy: 78.75 val accuracy: 73.75 loss 0.734038\n",
      "0 Train accuracy: 87.8125 val accuracy: 84.3533333333 loss 0.374085\n",
      "0 Train accuracy: 95.3125 val accuracy: 88.26 loss 0.188541\n",
      "0 Train accuracy: 96.875 val accuracy: 89.5833333333 loss 0.243734\n",
      "0 Train accuracy: 97.5 val accuracy: 91.0533333333 loss 0.101325\n",
      "0 Train accuracy: 96.25 val accuracy: 91.6366666667 loss 0.200216\n",
      "0 Train accuracy: 95.9375 val accuracy: 91.5866666667 loss 0.186325\n",
      "1 Train accuracy: 96.875 val accuracy: 92.2566666667 loss 0.140618\n",
      "1 Train accuracy: 98.4375 val accuracy: 92.5866666667 loss 0.0843248\n",
      "1 Train accuracy: 95.9375 val accuracy: 92.81 loss 0.224882\n",
      "1 Train accuracy: 97.1875 val accuracy: 93.04 loss 0.157401\n",
      "1 Train accuracy: 99.375 val accuracy: 93.0133333333 loss 0.13217\n",
      "1 Train accuracy: 98.4375 val accuracy: 93.4633333333 loss 0.0677942\n",
      "1 Train accuracy: 97.8125 val accuracy: 93.6166666667 loss 0.181758\n",
      "1 Train accuracy: 97.8125 val accuracy: 93.5966666667 loss 0.125267\n",
      "2 Train accuracy: 97.5 val accuracy: 93.74 loss 0.120467\n",
      "2 Train accuracy: 99.375 val accuracy: 93.92 loss 0.0355982\n",
      "2 Train accuracy: 95.9375 val accuracy: 93.8466666667 loss 0.178101\n",
      "2 Train accuracy: 97.8125 val accuracy: 93.96 loss 0.136844\n",
      "2 Train accuracy: 99.375 val accuracy: 94.0066666667 loss 0.097614\n",
      "2 Train accuracy: 99.0625 val accuracy: 94.3033333333 loss 0.0547412\n",
      "2 Train accuracy: 98.125 val accuracy: 94.21 loss 0.157978\n",
      "2 Train accuracy: 97.8125 val accuracy: 94.4033333333 loss 0.100858\n",
      "3 Train accuracy: 98.125 val accuracy: 94.3766666667 loss 0.110996\n",
      "3 Train accuracy: 99.375 val accuracy: 94.3466666667 loss 0.0291359\n",
      "3 Train accuracy: 96.875 val accuracy: 94.4866666667 loss 0.161386\n",
      "3 Train accuracy: 98.125 val accuracy: 94.5933333333 loss 0.125783\n",
      "3 Train accuracy: 99.6875 val accuracy: 94.6266666667 loss 0.0778615\n",
      "3 Train accuracy: 99.0625 val accuracy: 94.6566666667 loss 0.0451246\n",
      "3 Train accuracy: 98.125 val accuracy: 94.6033333333 loss 0.138668\n",
      "3 Train accuracy: 97.8125 val accuracy: 94.6166666667 loss 0.0675828\n",
      "4 Train accuracy: 98.125 val accuracy: 94.6733333333 loss 0.0881111\n",
      "4 Train accuracy: 99.375 val accuracy: 94.7333333333 loss 0.0290586\n",
      "4 Train accuracy: 97.1875 val accuracy: 94.7766666667 loss 0.164346\n",
      "4 Train accuracy: 98.4375 val accuracy: 94.8033333333 loss 0.109982\n",
      "4 Train accuracy: 99.375 val accuracy: 94.8733333333 loss 0.0736806\n",
      "4 Train accuracy: 99.375 val accuracy: 94.8333333333 loss 0.0414983\n",
      "4 Train accuracy: 98.75 val accuracy: 94.76 loss 0.127326\n",
      "4 Train accuracy: 98.75 val accuracy: 94.91 loss 0.0476619\n",
      "5 Train accuracy: 98.125 val accuracy: 95.0 loss 0.0867308\n",
      "5 Train accuracy: 99.375 val accuracy: 94.91 loss 0.0277529\n",
      "5 Train accuracy: 96.875 val accuracy: 94.88 loss 0.162894\n",
      "5 Train accuracy: 98.75 val accuracy: 95.0 loss 0.100986\n",
      "5 Train accuracy: 99.6875 val accuracy: 94.96 loss 0.0735323\n",
      "5 Train accuracy: 99.375 val accuracy: 95.04 loss 0.040529\n",
      "5 Train accuracy: 99.0625 val accuracy: 94.9233333333 loss 0.120316\n",
      "5 Train accuracy: 98.4375 val accuracy: 94.9966666667 loss 0.0421199\n",
      "acc_test 93.9700030609\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 6\n",
    "batch_size = 64\n",
    "myloss=0.0\n",
    "with tf.Session(graph=mygraph1) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(train_dataset)// batch_size):\n",
    "            offset = (iteration * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            X_batch = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            y_batch = train_labels[offset:(offset + batch_size),:]\n",
    "            sess.run([training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "            if iteration %500 ==0:\n",
    "                train_pre = predictions.eval(feed_dict={X:X_batch})\n",
    "                #print(type(train_pre))\n",
    "                val_pre = predictions.eval(feed_dict={X:valid_dataset})     \n",
    "                #print(type(val_pre))\n",
    "                acc_val = accuracy(val_pre, valid_labels[:,1:6])\n",
    "                acc_train = accuracy(train_pre,y_batch[:,1:6])\n",
    "                myloss = loss1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                print(epoch, \"Train accuracy:\", acc_train, \"val accuracy:\", acc_val,\"loss\",myloss)\n",
    "\n",
    "        save_path = saver.save(sess, \".ipynb_checkpoints/3.ckpt\")\n",
    "    test_prediction = sess.run(predictions, feed_dict={X:test_dataset})\n",
    "    acc_test = accuracy(test_prediction,test_labels[:,1:6])  \n",
    "    print('acc_test',acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that's add a drop out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "mygraph1 = tf.Graph()\n",
    "\n",
    "with mygraph1.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None,height, width, channels], name=\"X\")\n",
    "    #X_reshaped = tf.reshape(X, shape=[-1, ])\n",
    "    y = tf.placeholder(tf.int32, shape=[None,6], name=\"y\")\n",
    "    training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    conv1 = tf.layers.conv2d(X, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "    pool1 = tf.layers.max_pooling2d(conv1,pool_size=2,strides=2,padding='same')\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "    pool2 = tf.layers.max_pooling2d(conv2,pool_size=2,strides=2,padding='same')\n",
    "    conv3 = tf.layers.conv2d(pool2,filters=128,kernel_size=conv2_ksize,\n",
    "                        strides=conv2_stride,padding=conv2_pad)\n",
    "    pool3 = tf.layers.max_pooling2d(conv3, 2,2, padding=\"same\")\n",
    "\n",
    "    conv4 = tf.layers.conv2d(pool3,filters=256,kernel_size=conv2_ksize,\n",
    "                        strides=conv2_stride,padding=conv2_pad,activation=tf.nn.relu)\n",
    "    pool4 = tf.layers.max_pooling2d(conv4, 2,2, padding=\"same\")\n",
    "    pool4_flat = tf.layers.flatten(pool4)\n",
    "   # pool4_flat = tf.layers.dropout(pool4_flat, training=training)\n",
    "    fc1 = tf.layers.dense(pool4_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "    fc1 = tf.layers.dropout(fc1, training=training)\n",
    "    logits1 = tf.layers.dense(fc1, n_outputs, name=\"output1\")\n",
    "    logits2 = tf.layers.dense(fc1, n_outputs, name=\"output2\")\n",
    "    logits3 = tf.layers.dense(fc1, n_outputs, name=\"output3\")\n",
    "    logits4 = tf.layers.dense(fc1, n_outputs, name=\"output4\")\n",
    "    logits5 = tf.layers.dense(fc1, n_outputs, name=\"output5\")\n",
    "    loss1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits1, labels=y[:,1]))\n",
    "    loss2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits2, labels=y[:,2]))\n",
    "    loss3 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits3, labels=y[:,3]))\n",
    "    loss4 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits4, labels=y[:,4]))\n",
    "    loss5 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits5, labels=y[:,5]))\n",
    "    loss = loss1+loss2+loss3+loss4+loss5\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "    training_op = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=5e-4)\n",
    "    #training_op = optimizer.minimize(loss)\n",
    "    predictions = tf.stack([tf.nn.softmax(logits1),\\\n",
    "                                tf.nn.softmax(logits2),\\\n",
    "                                tf.nn.softmax(logits3),\\\n",
    "                                tf.nn.softmax(logits4),\\\n",
    "                                tf.nn.softmax(logits5)])\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 51.25 val accuracy: 50.5666666667 loss 2.38419\n",
      "0 Train accuracy: 66.25 val accuracy: 61.7566666667 loss 1.47546\n",
      "0 Train accuracy: 77.1875 val accuracy: 75.71 loss 0.757426\n",
      "0 Train accuracy: 86.5625 val accuracy: 81.6266666667 loss 0.512815\n",
      "0 Train accuracy: 91.5625 val accuracy: 84.6966666667 loss 0.494119\n",
      "0 Train accuracy: 89.0625 val accuracy: 86.82 loss 0.384874\n",
      "0 Train accuracy: 90.3125 val accuracy: 87.9633333333 loss 0.383948\n",
      "0 Train accuracy: 89.6875 val accuracy: 88.9166666667 loss 0.313967\n",
      "1 Train accuracy: 91.875 val accuracy: 88.9166666667 loss 0.226047\n",
      "1 Train accuracy: 93.75 val accuracy: 89.5166666667 loss 0.157038\n",
      "1 Train accuracy: 88.4375 val accuracy: 90.34 loss 0.419954\n",
      "1 Train accuracy: 95.3125 val accuracy: 90.4033333333 loss 0.17741\n",
      "1 Train accuracy: 94.0625 val accuracy: 90.9533333333 loss 0.27696\n",
      "1 Train accuracy: 93.75 val accuracy: 91.28 loss 0.149484\n",
      "1 Train accuracy: 94.6875 val accuracy: 91.68 loss 0.247669\n",
      "1 Train accuracy: 92.5 val accuracy: 91.78 loss 0.173934\n",
      "2 Train accuracy: 93.75 val accuracy: 91.9733333333 loss 0.134509\n",
      "2 Train accuracy: 97.8125 val accuracy: 92.0 loss 0.104557\n",
      "2 Train accuracy: 92.1875 val accuracy: 92.2566666667 loss 0.290987\n",
      "2 Train accuracy: 95.9375 val accuracy: 92.54 loss 0.153706\n",
      "2 Train accuracy: 94.6875 val accuracy: 92.67 loss 0.265333\n",
      "2 Train accuracy: 96.5625 val accuracy: 92.6166666667 loss 0.105162\n",
      "2 Train accuracy: 94.0625 val accuracy: 92.96 loss 0.244374\n",
      "2 Train accuracy: 94.0625 val accuracy: 93.0133333333 loss 0.1658\n",
      "3 Train accuracy: 95.0 val accuracy: 93.01 loss 0.118966\n",
      "3 Train accuracy: 96.875 val accuracy: 93.1933333333 loss 0.0836336\n",
      "3 Train accuracy: 93.4375 val accuracy: 93.1033333333 loss 0.262372\n",
      "3 Train accuracy: 95.9375 val accuracy: 93.1833333333 loss 0.134058\n",
      "3 Train accuracy: 96.25 val accuracy: 93.3633333333 loss 0.21407\n",
      "3 Train accuracy: 96.875 val accuracy: 93.4333333333 loss 0.0936129\n",
      "3 Train accuracy: 93.75 val accuracy: 93.4233333333 loss 0.219577\n",
      "3 Train accuracy: 94.0625 val accuracy: 93.5733333333 loss 0.137515\n",
      "4 Train accuracy: 95.9375 val accuracy: 93.54 loss 0.105576\n",
      "4 Train accuracy: 97.5 val accuracy: 93.7133333333 loss 0.0851554\n",
      "4 Train accuracy: 94.0625 val accuracy: 93.61 loss 0.224636\n",
      "4 Train accuracy: 95.9375 val accuracy: 93.7433333333 loss 0.151045\n",
      "4 Train accuracy: 97.1875 val accuracy: 93.8266666667 loss 0.216935\n",
      "4 Train accuracy: 97.5 val accuracy: 93.9066666667 loss 0.0726451\n",
      "4 Train accuracy: 95.9375 val accuracy: 93.9033333333 loss 0.185211\n",
      "4 Train accuracy: 95.3125 val accuracy: 94.0 loss 0.139849\n",
      "5 Train accuracy: 95.625 val accuracy: 93.91 loss 0.105034\n",
      "5 Train accuracy: 98.4375 val accuracy: 94.1366666667 loss 0.0869597\n",
      "5 Train accuracy: 94.6875 val accuracy: 94.16 loss 0.226963\n",
      "5 Train accuracy: 96.25 val accuracy: 94.19 loss 0.137893\n",
      "5 Train accuracy: 96.25 val accuracy: 94.0933333333 loss 0.18674\n",
      "5 Train accuracy: 97.1875 val accuracy: 94.1633333333 loss 0.0731261\n",
      "5 Train accuracy: 97.1875 val accuracy: 94.2633333333 loss 0.242554\n",
      "5 Train accuracy: 95.625 val accuracy: 94.2433333333 loss 0.115522\n",
      "acc_test 94.2393633303\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 6\n",
    "batch_size = 64\n",
    "myloss=0.0\n",
    "with tf.Session(graph=mygraph1) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(train_dataset)// batch_size):\n",
    "            offset = (iteration * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            X_batch = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            y_batch = train_labels[offset:(offset + batch_size),:]\n",
    "            sess.run([training_op], feed_dict={X: X_batch, y: y_batch, training:True})\n",
    "            \n",
    "            if iteration %500 ==0:\n",
    "                train_pre = predictions.eval(feed_dict={X:X_batch})\n",
    "                #print(type(train_pre))\n",
    "                val_pre = predictions.eval(feed_dict={X:valid_dataset})     \n",
    "                #print(type(val_pre))\n",
    "                acc_val = accuracy(val_pre, valid_labels[:,1:6])\n",
    "                acc_train = accuracy(train_pre,y_batch[:,1:6])\n",
    "                myloss = loss1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                print(epoch, \"Train accuracy:\", acc_train, \"val accuracy:\", acc_val,\"loss\",myloss)\n",
    "\n",
    "        save_path = saver.save(sess, \".ipynb_checkpoints/3.ckpt\")\n",
    "    test_prediction = sess.run(predictions, feed_dict={X:test_dataset})\n",
    "    acc_test = accuracy(test_prediction,test_labels[:,1:6])  \n",
    "    print('acc_test',acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add local norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph() \n",
    "mygraph1 = tf.Graph()\n",
    "\n",
    "with mygraph1.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None,height, width, channels], name=\"X\")\n",
    "    #X_reshaped = tf.reshape(X, shape=[-1, ])\n",
    "    y = tf.placeholder(tf.int32, shape=[None,6], name=\"y\")\n",
    "    training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    conv1 = tf.layers.conv2d(X, filters=32, kernel_size=5,strides=1,activation=tf.nn.relu)              \n",
    "    conv1 = tf.nn.local_response_normalization(conv1)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1,pool_size=2,strides=2,padding='same')\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=64, kernel_size=5,strides=1,activation=tf.nn.relu)\n",
    "    conv2 = tf.nn.local_response_normalization(conv2)\n",
    "    pool2 = tf.layers.max_pooling2d(conv2,pool_size=2,strides=2,padding='same')\n",
    "    conv3 = tf.layers.conv2d(pool2,filters=512,kernel_size=5,strides=1,activation=tf.nn.relu)\n",
    "    conv3 = tf.nn.local_response_normalization(conv3)\n",
    "   \n",
    "    pool4_flat = tf.layers.dropout(conv3, rate=0.75,training=training)    \n",
    "    pool4_flat = tf.layers.flatten(pool4_flat)\n",
    "    \n",
    "    fc1 = tf.layers.dense(pool4_flat, 256, activation=tf.nn.relu)\n",
    "    fc1 = tf.layers.dropout(fc1,  rate=0.75,training=training)\n",
    "    logits1 = tf.layers.dense(fc1, n_outputs, name=\"output1\")\n",
    "    logits2 = tf.layers.dense(fc1, n_outputs, name=\"output2\")\n",
    "    logits3 = tf.layers.dense(fc1, n_outputs, name=\"output3\")\n",
    "    logits4 = tf.layers.dense(fc1, n_outputs, name=\"output4\")\n",
    "    logits5 = tf.layers.dense(fc1, n_outputs, name=\"output5\")\n",
    "    loss1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits1, labels=y[:,1]))\n",
    "    loss2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits2, labels=y[:,2]))\n",
    "    loss3 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits3, labels=y[:,3]))\n",
    "    loss4 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits4, labels=y[:,4]))\n",
    "    loss5 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits5, labels=y[:,5]))\n",
    "    loss = loss1+loss2+loss3+loss4+loss5\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "    training_op = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=5e-4)\n",
    "    #training_op = optimizer.minimize(loss)\n",
    "    predictions = tf.stack([tf.nn.softmax(logits1),\\\n",
    "                                tf.nn.softmax(logits2),\\\n",
    "                                tf.nn.softmax(logits3),\\\n",
    "                                tf.nn.softmax(logits4),\\\n",
    "                                tf.nn.softmax(logits5)])\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 54.6875 val accuracy: 52.12 loss 2.30427\n",
      "0 Train accuracy: 63.75 val accuracy: 63.9033333333 loss 1.44722\n",
      "0 Train accuracy: 73.4375 val accuracy: 73.9133333333 loss 0.958066\n",
      "0 Train accuracy: 81.25 val accuracy: 78.5033333333 loss 0.737397\n",
      "0 Train accuracy: 87.8125 val accuracy: 81.4433333333 loss 0.662165\n",
      "0 Train accuracy: 84.0625 val accuracy: 82.6066666667 loss 0.70383\n",
      "0 Train accuracy: 89.0625 val accuracy: 84.8633333333 loss 0.656467\n",
      "0 Train accuracy: 86.25 val accuracy: 86.5266666667 loss 0.582105\n",
      "1 Train accuracy: 86.875 val accuracy: 86.47 loss 0.523575\n",
      "1 Train accuracy: 92.5 val accuracy: 87.9666666667 loss 0.430324\n",
      "1 Train accuracy: 87.1875 val accuracy: 88.6366666667 loss 0.614644\n",
      "1 Train accuracy: 90.0 val accuracy: 89.22 loss 0.375707\n",
      "1 Train accuracy: 95.0 val accuracy: 89.8566666667 loss 0.324665\n",
      "1 Train accuracy: 91.25 val accuracy: 90.3533333333 loss 0.390957\n",
      "1 Train accuracy: 92.8125 val accuracy: 90.6733333333 loss 0.390859\n",
      "1 Train accuracy: 91.5625 val accuracy: 91.16 loss 0.369802\n",
      "2 Train accuracy: 92.5 val accuracy: 91.1533333333 loss 0.282611\n",
      "2 Train accuracy: 95.625 val accuracy: 91.7633333333 loss 0.223942\n",
      "2 Train accuracy: 89.6875 val accuracy: 91.7833333333 loss 0.452604\n",
      "2 Train accuracy: 94.375 val accuracy: 92.1966666667 loss 0.259341\n",
      "2 Train accuracy: 97.1875 val accuracy: 92.31 loss 0.249924\n",
      "2 Train accuracy: 95.0 val accuracy: 92.4133333333 loss 0.27035\n",
      "2 Train accuracy: 92.8125 val accuracy: 92.4766666667 loss 0.273424\n",
      "2 Train accuracy: 93.4375 val accuracy: 92.68 loss 0.27574\n",
      "3 Train accuracy: 94.0625 val accuracy: 92.88 loss 0.193184\n",
      "3 Train accuracy: 95.9375 val accuracy: 92.8033333333 loss 0.18701\n",
      "3 Train accuracy: 90.9375 val accuracy: 93.1 loss 0.431594\n",
      "3 Train accuracy: 95.3125 val accuracy: 93.07 loss 0.231558\n",
      "3 Train accuracy: 96.25 val accuracy: 93.2366666667 loss 0.207039\n",
      "3 Train accuracy: 95.9375 val accuracy: 93.35 loss 0.181666\n",
      "3 Train accuracy: 94.6875 val accuracy: 93.2766666667 loss 0.218006\n",
      "3 Train accuracy: 93.75 val accuracy: 93.3 loss 0.246382\n",
      "4 Train accuracy: 93.75 val accuracy: 93.4566666667 loss 0.157084\n",
      "4 Train accuracy: 95.9375 val accuracy: 93.49 loss 0.138645\n",
      "4 Train accuracy: 93.125 val accuracy: 93.48 loss 0.408197\n",
      "4 Train accuracy: 95.3125 val accuracy: 93.65 loss 0.197351\n",
      "4 Train accuracy: 98.125 val accuracy: 93.7666666667 loss 0.132227\n",
      "4 Train accuracy: 95.3125 val accuracy: 93.8066666667 loss 0.190274\n",
      "4 Train accuracy: 94.375 val accuracy: 93.7466666667 loss 0.196279\n",
      "4 Train accuracy: 94.0625 val accuracy: 93.8833333333 loss 0.223377\n",
      "5 Train accuracy: 94.6875 val accuracy: 94.0066666667 loss 0.189756\n",
      "5 Train accuracy: 96.25 val accuracy: 94.1033333333 loss 0.146849\n",
      "5 Train accuracy: 92.8125 val accuracy: 93.9133333333 loss 0.352223\n",
      "5 Train accuracy: 96.5625 val accuracy: 94.0433333333 loss 0.169385\n",
      "5 Train accuracy: 97.8125 val accuracy: 94.0733333333 loss 0.141501\n",
      "5 Train accuracy: 95.625 val accuracy: 94.2533333333 loss 0.166725\n",
      "5 Train accuracy: 94.6875 val accuracy: 94.16 loss 0.187401\n",
      "5 Train accuracy: 95.0 val accuracy: 94.2266666667 loss 0.174286\n",
      "6 Train accuracy: 94.6875 val accuracy: 94.2533333333 loss 0.155097\n",
      "6 Train accuracy: 96.5625 val accuracy: 94.2166666667 loss 0.116497\n",
      "6 Train accuracy: 94.0625 val accuracy: 94.17 loss 0.342553\n",
      "6 Train accuracy: 96.25 val accuracy: 94.3233333333 loss 0.165382\n",
      "6 Train accuracy: 97.8125 val accuracy: 94.2633333333 loss 0.147594\n",
      "6 Train accuracy: 95.9375 val accuracy: 94.3766666667 loss 0.166673\n",
      "6 Train accuracy: 93.75 val accuracy: 94.3766666667 loss 0.212575\n",
      "6 Train accuracy: 95.3125 val accuracy: 94.4133333333 loss 0.186946\n",
      "7 Train accuracy: 95.3125 val accuracy: 94.4633333333 loss 0.117155\n",
      "7 Train accuracy: 96.5625 val accuracy: 94.5066666667 loss 0.108836\n",
      "7 Train accuracy: 95.3125 val accuracy: 94.51 loss 0.326867\n",
      "7 Train accuracy: 95.625 val accuracy: 94.5766666667 loss 0.158414\n",
      "7 Train accuracy: 97.8125 val accuracy: 94.62 loss 0.0904858\n",
      "7 Train accuracy: 96.5625 val accuracy: 94.5766666667 loss 0.13212\n",
      "7 Train accuracy: 94.0625 val accuracy: 94.5733333333 loss 0.223242\n",
      "7 Train accuracy: 95.3125 val accuracy: 94.6133333333 loss 0.218021\n",
      "8 Train accuracy: 95.9375 val accuracy: 94.68 loss 0.103391\n",
      "8 Train accuracy: 96.5625 val accuracy: 94.64 loss 0.108247\n",
      "8 Train accuracy: 95.0 val accuracy: 94.6533333333 loss 0.320388\n",
      "8 Train accuracy: 95.9375 val accuracy: 94.8033333333 loss 0.178529\n",
      "8 Train accuracy: 98.4375 val accuracy: 94.6 loss 0.0969308\n",
      "8 Train accuracy: 96.25 val accuracy: 94.7666666667 loss 0.147968\n",
      "8 Train accuracy: 94.0625 val accuracy: 94.6966666667 loss 0.203323\n",
      "8 Train accuracy: 95.9375 val accuracy: 94.8233333333 loss 0.189185\n",
      "9 Train accuracy: 95.0 val accuracy: 94.8133333333 loss 0.139046\n",
      "9 Train accuracy: 96.5625 val accuracy: 94.7166666667 loss 0.100157\n",
      "9 Train accuracy: 95.3125 val accuracy: 94.7433333333 loss 0.278121\n",
      "9 Train accuracy: 96.5625 val accuracy: 94.79 loss 0.167517\n",
      "9 Train accuracy: 97.5 val accuracy: 94.7566666667 loss 0.114286\n",
      "9 Train accuracy: 95.9375 val accuracy: 94.83 loss 0.173999\n",
      "9 Train accuracy: 94.6875 val accuracy: 94.8 loss 0.182379\n",
      "9 Train accuracy: 95.3125 val accuracy: 94.9033333333 loss 0.170481\n",
      "acc_test 95.382614019\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "myloss=0.0\n",
    "with tf.Session(graph=mygraph1) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(train_dataset)// batch_size):\n",
    "            offset = (iteration * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            X_batch = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            y_batch = train_labels[offset:(offset + batch_size),:]\n",
    "            sess.run([training_op], feed_dict={X: X_batch, y: y_batch, training:True})\n",
    "            \n",
    "            if iteration %500 ==0:\n",
    "                train_pre = predictions.eval(feed_dict={X:X_batch})\n",
    "                #print(type(train_pre))\n",
    "                val_pre = predictions.eval(feed_dict={X:valid_dataset})     \n",
    "                #print(type(val_pre))\n",
    "                acc_val = accuracy(val_pre, valid_labels[:,1:6])\n",
    "                acc_train = accuracy(train_pre,y_batch[:,1:6])\n",
    "                myloss = loss1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                print(epoch, \"Train accuracy:\", acc_train, \"val accuracy:\", acc_val,\"loss\",myloss)\n",
    "\n",
    "        save_path = saver.save(sess, \".ipynb_checkpoints/4.ckpt\")\n",
    "    test_prediction = sess.run(predictions, feed_dict={X:test_dataset})\n",
    "    acc_test = accuracy(test_prediction,test_labels[:,1:6])  \n",
    "    print('acc_test',acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph() \n",
    "mygraph1 = tf.Graph()\n",
    "\n",
    "with mygraph1.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None,height, width, channels], name=\"X\")\n",
    "    #X_reshaped = tf.reshape(X, shape=[-1, ])\n",
    "    y = tf.placeholder(tf.int32, shape=[None,6], name=\"y\")\n",
    "    training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    conv1 = tf.layers.conv2d(X, filters=32, kernel_size=5,strides=1,activation=tf.nn.relu)              \n",
    "    conv1 = tf.nn.local_response_normalization(conv1)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1,pool_size=2,strides=2,padding='same')\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=64, kernel_size=5,strides=1,activation=tf.nn.relu)\n",
    "    conv2 = tf.nn.local_response_normalization(conv2)\n",
    "    pool2 = tf.layers.max_pooling2d(conv2,pool_size=2,strides=2,padding='same')\n",
    "    conv3 = tf.layers.conv2d(pool2,filters=512,kernel_size=5,strides=1,activation=tf.nn.relu)\n",
    "    conv3 = tf.nn.local_response_normalization(conv3)\n",
    "   \n",
    "    #pool4_flat = tf.layers.dropout(conv3, rate=0.75,training=training)    \n",
    "    pool4_flat = tf.layers.flatten(conv3)\n",
    "    \n",
    "    fc1 = tf.layers.dense(pool4_flat, 256, activation=tf.nn.relu)\n",
    "    #fc1 = tf.layers.dropout(fc1,  rate=0.75,training=training)\n",
    "    logits1 = tf.layers.dense(fc1, n_outputs, name=\"output1\")\n",
    "    logits2 = tf.layers.dense(fc1, n_outputs, name=\"output2\")\n",
    "    logits3 = tf.layers.dense(fc1, n_outputs, name=\"output3\")\n",
    "    logits4 = tf.layers.dense(fc1, n_outputs, name=\"output4\")\n",
    "    logits5 = tf.layers.dense(fc1, n_outputs, name=\"output5\")\n",
    "    loss1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits1, labels=y[:,1]))\n",
    "    loss2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits2, labels=y[:,2]))\n",
    "    loss3 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits3, labels=y[:,3]))\n",
    "    loss4 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits4, labels=y[:,4]))\n",
    "    loss5 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits5, labels=y[:,5]))\n",
    "    loss = loss1+loss2+loss3+loss4+loss5\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.1, global_step, 10000, 0.95)\n",
    "    training_op = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=5e-4)\n",
    "    #training_op = optimizer.minimize(loss)\n",
    "    predictions = tf.stack([tf.nn.softmax(logits1),\\\n",
    "                                tf.nn.softmax(logits2),\\\n",
    "                                tf.nn.softmax(logits3),\\\n",
    "                                tf.nn.softmax(logits4),\\\n",
    "                                tf.nn.softmax(logits5)])\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 52.96875 val accuracy: 51.77 loss 2.25889\n",
      "0 Train accuracy: 89.6875 val accuracy: 84.4833333333 loss 0.496126\n",
      "0 Train accuracy: 96.5625 val accuracy: 91.2133333333 loss 0.15096\n",
      "0 Train accuracy: 97.96875 val accuracy: 93.6666666667 loss 0.081123\n",
      "1 Train accuracy: 99.0625 val accuracy: 94.3633333333 loss 0.035641\n",
      "1 Train accuracy: 98.59375 val accuracy: 94.89 loss 0.100151\n",
      "1 Train accuracy: 99.53125 val accuracy: 95.2766666667 loss 0.0638814\n",
      "1 Train accuracy: 99.6875 val accuracy: 95.78 loss 0.0449799\n",
      "2 Train accuracy: 99.53125 val accuracy: 95.7833333333 loss 0.0166259\n",
      "2 Train accuracy: 99.375 val accuracy: 95.92 loss 0.0471061\n",
      "2 Train accuracy: 99.6875 val accuracy: 95.9566666667 loss 0.0439\n",
      "2 Train accuracy: 99.6875 val accuracy: 96.22 loss 0.0255076\n",
      "3 Train accuracy: 100.0 val accuracy: 96.1266666667 loss 0.0109989\n",
      "3 Train accuracy: 99.6875 val accuracy: 96.26 loss 0.0310912\n",
      "3 Train accuracy: 99.84375 val accuracy: 96.2366666667 loss 0.0282972\n",
      "3 Train accuracy: 99.84375 val accuracy: 96.26 loss 0.0198284\n",
      "4 Train accuracy: 100.0 val accuracy: 96.37 loss 0.00884621\n",
      "4 Train accuracy: 99.84375 val accuracy: 96.4066666667 loss 0.0211617\n",
      "4 Train accuracy: 99.84375 val accuracy: 96.2933333333 loss 0.0219899\n",
      "4 Train accuracy: 100.0 val accuracy: 96.3533333333 loss 0.0136008\n",
      "5 Train accuracy: 100.0 val accuracy: 96.34 loss 0.00689374\n",
      "5 Train accuracy: 100.0 val accuracy: 96.4233333333 loss 0.0113993\n",
      "5 Train accuracy: 99.84375 val accuracy: 96.3766666667 loss 0.0144376\n",
      "5 Train accuracy: 100.0 val accuracy: 96.44 loss 0.0115227\n",
      "6 Train accuracy: 100.0 val accuracy: 96.4166666667 loss 0.00544019\n",
      "6 Train accuracy: 100.0 val accuracy: 96.35 loss 0.0097105\n",
      "6 Train accuracy: 100.0 val accuracy: 96.36 loss 0.0109204\n",
      "6 Train accuracy: 100.0 val accuracy: 96.43 loss 0.00765075\n",
      "7 Train accuracy: 100.0 val accuracy: 96.4366666667 loss 0.00323288\n",
      "7 Train accuracy: 100.0 val accuracy: 96.41 loss 0.00719879\n",
      "7 Train accuracy: 100.0 val accuracy: 96.4266666667 loss 0.00532444\n",
      "7 Train accuracy: 100.0 val accuracy: 96.3866666667 loss 0.00506549\n",
      "8 Train accuracy: 100.0 val accuracy: 96.5166666667 loss 0.00233421\n",
      "8 Train accuracy: 100.0 val accuracy: 96.4833333333 loss 0.00539278\n",
      "8 Train accuracy: 99.84375 val accuracy: 96.3766666667 loss 0.00362144\n",
      "8 Train accuracy: 100.0 val accuracy: 96.42 loss 0.00399484\n",
      "9 Train accuracy: 100.0 val accuracy: 96.5066666667 loss 0.00166208\n",
      "9 Train accuracy: 100.0 val accuracy: 96.5166666667 loss 0.00357105\n",
      "9 Train accuracy: 100.0 val accuracy: 96.46 loss 0.00189842\n",
      "9 Train accuracy: 100.0 val accuracy: 96.4833333333 loss 0.00249241\n",
      "10 Train accuracy: 100.0 val accuracy: 96.5833333333 loss 0.00152923\n",
      "10 Train accuracy: 100.0 val accuracy: 96.5266666667 loss 0.00235182\n",
      "10 Train accuracy: 100.0 val accuracy: 96.4566666667 loss 0.00250851\n",
      "10 Train accuracy: 100.0 val accuracy: 96.4433333333 loss 0.00221657\n",
      "11 Train accuracy: 100.0 val accuracy: 96.6066666667 loss 0.00150299\n",
      "11 Train accuracy: 100.0 val accuracy: 96.57 loss 0.00184406\n",
      "11 Train accuracy: 100.0 val accuracy: 96.5733333333 loss 0.00107136\n",
      "11 Train accuracy: 100.0 val accuracy: 96.48 loss 0.00190284\n",
      "12 Train accuracy: 100.0 val accuracy: 96.6666666667 loss 0.00107006\n",
      "12 Train accuracy: 100.0 val accuracy: 96.64 loss 0.00184088\n",
      "12 Train accuracy: 100.0 val accuracy: 96.5466666667 loss 0.000999604\n",
      "12 Train accuracy: 100.0 val accuracy: 96.6 loss 0.00154993\n",
      "13 Train accuracy: 100.0 val accuracy: 96.67 loss 0.000958124\n",
      "13 Train accuracy: 100.0 val accuracy: 96.66 loss 0.00134129\n",
      "13 Train accuracy: 100.0 val accuracy: 96.61 loss 0.000816575\n",
      "13 Train accuracy: 100.0 val accuracy: 96.5533333333 loss 0.00160316\n",
      "14 Train accuracy: 100.0 val accuracy: 96.7766666667 loss 0.000613391\n",
      "14 Train accuracy: 100.0 val accuracy: 96.6333333333 loss 0.00127768\n",
      "14 Train accuracy: 100.0 val accuracy: 96.6533333333 loss 0.000781543\n",
      "14 Train accuracy: 100.0 val accuracy: 96.61 loss 0.001456\n",
      "acc_test 95.8295071931\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "batch_size = 128\n",
    "myloss=0.0\n",
    "with tf.Session(graph=mygraph1) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(train_dataset)// batch_size):\n",
    "            offset = (iteration * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            X_batch = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            y_batch = train_labels[offset:(offset + batch_size),:]\n",
    "            sess.run([training_op], feed_dict={X: X_batch, y: y_batch, training:True})\n",
    "            \n",
    "            if iteration %500 ==0:\n",
    "                train_pre = predictions.eval(feed_dict={X:X_batch})\n",
    "                #print(type(train_pre))\n",
    "                val_pre = predictions.eval(feed_dict={X:valid_dataset})     \n",
    "                #print(type(val_pre))\n",
    "                acc_val = accuracy(val_pre, valid_labels[:,1:6])\n",
    "                acc_train = accuracy(train_pre,y_batch[:,1:6])\n",
    "                myloss = loss1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                print(epoch, \"Train accuracy:\", acc_train, \"val accuracy:\", acc_val,\"loss\",myloss)\n",
    "\n",
    "        save_path = saver.save(sess, \".ipynb_checkpoints/4.ckpt\")\n",
    "    test_prediction = sess.run(predictions, feed_dict={X:test_dataset})\n",
    "    acc_test = accuracy(test_prediction,test_labels[:,1:6])  \n",
    "    print('acc_test',acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overfiting  when add batch norm,it will decrease the acc,so drop the batch_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 56.0 val accuracy: 55.14 loss 2.05757\n",
      "0 Train accuracy: 90.6 val accuracy: 82.2966666667 loss 0.401906\n",
      "0 Train accuracy: 96.8 val accuracy: 90.5866666667 loss 0.206408\n",
      "0 Train accuracy: 98.4 val accuracy: 92.6733333333 loss 0.100631\n",
      "0 Train accuracy: 98.4 val accuracy: 93.8066666667 loss 0.0576476\n",
      "1 Train accuracy: 99.2 val accuracy: 94.1633333333 loss 0.0387746\n",
      "1 Train accuracy: 99.0 val accuracy: 94.7633333333 loss 0.0497186\n",
      "1 Train accuracy: 99.4 val accuracy: 95.07 loss 0.110827\n",
      "1 Train accuracy: 99.0 val accuracy: 95.39 loss 0.0713903\n",
      "1 Train accuracy: 99.4 val accuracy: 95.5633333333 loss 0.0347085\n",
      "2 Train accuracy: 100.0 val accuracy: 95.5666666667 loss 0.0234515\n",
      "2 Train accuracy: 99.6 val accuracy: 95.8466666667 loss 0.0273821\n",
      "2 Train accuracy: 99.8 val accuracy: 95.6466666667 loss 0.0700638\n",
      "2 Train accuracy: 99.2 val accuracy: 95.8766666667 loss 0.0469879\n",
      "2 Train accuracy: 99.4 val accuracy: 96.0433333333 loss 0.0248838\n",
      "3 Train accuracy: 99.8 val accuracy: 95.94 loss 0.015556\n",
      "3 Train accuracy: 99.6 val accuracy: 96.0933333333 loss 0.0142742\n",
      "3 Train accuracy: 99.8 val accuracy: 96.0266666667 loss 0.0280418\n",
      "3 Train accuracy: 99.6 val accuracy: 96.1733333333 loss 0.0282776\n",
      "3 Train accuracy: 99.6 val accuracy: 96.2133333333 loss 0.0153734\n",
      "4 Train accuracy: 100.0 val accuracy: 95.9766666667 loss 0.0112081\n",
      "4 Train accuracy: 99.8 val accuracy: 96.1066666667 loss 0.0107809\n",
      "4 Train accuracy: 100.0 val accuracy: 96.1 loss 0.0118359\n",
      "4 Train accuracy: 99.6 val accuracy: 96.2433333333 loss 0.019303\n",
      "4 Train accuracy: 99.8 val accuracy: 96.2033333333 loss 0.0105396\n",
      "5 Train accuracy: 100.0 val accuracy: 96.1433333333 loss 0.00732949\n",
      "5 Train accuracy: 99.8 val accuracy: 96.03 loss 0.00660604\n",
      "5 Train accuracy: 100.0 val accuracy: 96.1466666667 loss 0.00585849\n",
      "acc_test 95.3290480563\n",
      "acc_test 95.4545454545\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 6\n",
    "batch_size = 100\n",
    "myloss=0.0\n",
    "best_acc =0.0\n",
    "n=10\n",
    "with tf.Session(graph=mygraph1) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        if n==0:\n",
    "            break\n",
    "        for iteration in range(len(train_dataset)// batch_size):\n",
    "            if n==0:\n",
    "                break\n",
    "            offset = (iteration * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            X_batch = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            y_batch = train_labels[offset:(offset + batch_size),:]\n",
    "            sess.run([training_op], feed_dict={X: X_batch, y: y_batch, training:True})\n",
    "            \n",
    "            if iteration %500 ==0:\n",
    "                train_pre = predictions.eval(feed_dict={X:X_batch})\n",
    "                #print(type(train_pre))\n",
    "                val_pre = predictions.eval(feed_dict={X:valid_dataset})     \n",
    "                #print(type(val_pre))\n",
    "                acc_val = accuracy(val_pre, valid_labels[:,1:6])\n",
    "                acc_train = accuracy(train_pre,y_batch[:,1:6])\n",
    "                myloss = loss1.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                print(epoch, \"Train accuracy:\", acc_train, \"val accuracy:\", acc_val,\"loss\",myloss)\n",
    "                if acc_val>best_acc:\n",
    "                    best_acc = acc_val\n",
    "                if acc_val<best_acc:\n",
    "                    n -=1\n",
    "                    if n==0:\n",
    "                        test_prediction = sess.run(predictions, feed_dict={X:test_dataset})\n",
    "                        print('acc_test',acc_test)\n",
    "                        save_path = saver.save(sess, \".ipynb_checkpoints/4.ckpt\")\n",
    "                \n",
    "        save_path = saver.save(sess, \".ipynb_checkpoints/4.ckpt\")\n",
    "    test_prediction = sess.run(predictions, feed_dict={X:test_dataset})\n",
    "    acc_test = accuracy(test_prediction,test_labels[:,1:6])  \n",
    "    print('acc_test',acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the length information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph() \n",
    "mygraph1 = tf.Graph()\n",
    "\n",
    "with mygraph1.as_default():\n",
    "    X = tf.placeholder(tf.float32, shape=[None,height, width, channels], name=\"X\")\n",
    "    #X_reshaped = tf.reshape(X, shape=[-1, ])\n",
    "    y = tf.placeholder(tf.int32, shape=[None,6], name=\"y\")\n",
    "    training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "    #he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    layer = tf.layers.conv2d(X, filters=48, kernel_size=5,strides=1,padding='same',activation=tf.nn.relu)              \n",
    "    layer = tf.nn.local_response_normalization(layer)\n",
    "    layer = tf.layers.max_pooling2d(layer,pool_size=2,strides=2,padding='same')\n",
    "    #layer = tf.layers.dropout(layer,0.2)\n",
    "    layer = tf.layers.conv2d(layer, filters=64, kernel_size=5,strides=1,padding='same',activation=tf.nn.relu)              \n",
    "    layer = tf.nn.local_response_normalization(layer)\n",
    "    layer = tf.layers.max_pooling2d(layer,pool_size=2,strides=2,padding='same')\n",
    "    #layer = tf.layers.dropout(layer,0.2)\n",
    "    layer = tf.layers.conv2d(layer, filters=128, kernel_size=5,strides=1,padding='same',activation=tf.nn.relu)              \n",
    "    layer = tf.nn.local_response_normalization(layer)\n",
    "    layer = tf.layers.max_pooling2d(layer,pool_size=2,strides=2,padding='same')\n",
    "    #layer = tf.layers.dropout(layer,0.2)\n",
    "    layer = tf.layers.conv2d(layer, filters=160, kernel_size=5,strides=1,padding='same',activation=tf.nn.relu)              \n",
    "    layer = tf.nn.local_response_normalization(layer)\n",
    "    layer = tf.layers.max_pooling2d(layer,pool_size=2,strides=2,padding='same')   \n",
    "    #layer = tf.layers.dropout(layer,0.2)\n",
    "    layer = tf.layers.conv2d(layer, filters=192, kernel_size=5,strides=1,padding='same',activation=tf.nn.relu)              \n",
    "    layer = tf.nn.local_response_normalization(layer)\n",
    "    layer = tf.layers.max_pooling2d(layer,pool_size=2,strides=2,padding='same')\n",
    "    #layer = tf.layers.dropout(layer,0.2)\n",
    "   # layer = tf.layers.conv2d(layer, filters=192, kernel_size=5,strides=1,padding='same',activation=tf.nn.relu)              \n",
    "    #layer = tf.nn.local_response_normalization(layer)\n",
    "    #layer = tf.layers.max_pooling2d(layer,pool_size=2,strides=2,padding='same')  \n",
    "    #layer = tf.layers.dropout(layer,0.2)\n",
    "    #layer = tf.layers.conv2d(layer, filters=192, kernel_size=5,strides=1,padding='same',activation=tf.nn.relu)              \n",
    "    #layer = tf.nn.local_response_normalization(layer)\n",
    "    #layer = tf.layers.max_pooling2d(layer,pool_size=2,strides=2,padding='same')\n",
    "    #layer = tf.layers.dropout(layer,0.2)\n",
    "    #layer = tf.layers.conv2d(layer, filters=192, kernel_size=5,strides=1,padding='same',activation=tf.nn.relu)              \n",
    "    #layer = tf.nn.local_response_normalization(layer)\n",
    "    #layer = tf.layers.max_pooling2d(layer,pool_size=2,strides=2,padding='same')  \n",
    "    #layer = tf.layers.dropout(layer,0.2)\n",
    "    #pool4_flat = tf.layers.dropout(conv3, rate=0.75,training=training)    \n",
    "    layer = tf.layers.flatten(layer)\n",
    "    layer = tf.layers.dense(layer,3072,activation=tf.nn.relu)\n",
    "    layer = tf.layers.dense(layer,3072,activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "    #fc1 = tf.layers.dropout(fc1,  rate=0.75,training=training)\n",
    "    logits0 = tf.layers.dense(layer, n_outputs, name='output0')\n",
    "    logits1 = tf.layers.dense(layer, n_outputs, name=\"output1\")\n",
    "    logits2 = tf.layers.dense(layer, n_outputs, name=\"output2\")\n",
    "    logits3 = tf.layers.dense(layer, n_outputs, name=\"output3\")\n",
    "    logits4 = tf.layers.dense(layer, n_outputs, name=\"output4\")\n",
    "    logits5 = tf.layers.dense(layer, n_outputs, name=\"output5\")\n",
    "    loss0 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits0, labels=y[:,0]))\n",
    "    loss1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits1, labels=y[:,1]))\n",
    "    loss2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits2, labels=y[:,2]))\n",
    "    loss3 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits3, labels=y[:,3]))\n",
    "    loss4 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits4, labels=y[:,4]))\n",
    "    loss5 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits5, labels=y[:,5]))\n",
    "    loss = loss1+loss2+loss3+loss4+loss5 + loss0\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.1, global_step, 10000, 0.95)\n",
    "    training_op = tf.train.AdamOptimizer().minimize(loss) #,global_step=global_step\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=5e-4)\n",
    "    #training_op = optimizer.minimize(loss)\n",
    "    predictions = tf.stack([tf.nn.softmax(logits0),\\\n",
    "                                tf.nn.softmax(logits1),\\\n",
    "                                tf.nn.softmax(logits2),\\\n",
    "                                tf.nn.softmax(logits3),\\\n",
    "                                tf.nn.softmax(logits4),\\\n",
    "                                tf.nn.softmax(logits5)])\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 55.0 val accuracy: 53.9583333333 loss 0.0226749777794\n",
      "0 Train accuracy: 65.0 val accuracy: 64.2472222222 loss 0.0184392404556\n",
      "0 Train accuracy: 81.3333333333 val accuracy: 77.8027777778 loss 0.010255573988\n",
      "0 Train accuracy: 90.0 val accuracy: 88.0416666667 loss 0.00455188035965\n",
      "0 Train accuracy: 91.3333333333 val accuracy: 92.1027777778 loss 0.00270106881857\n",
      "1 Train accuracy: 93.3333333333 val accuracy: 92.9416666667 loss 0.00259124308825\n",
      "1 Train accuracy: 95.1666666667 val accuracy: 93.5861111111 loss 0.00186678394675\n",
      "1 Train accuracy: 97.0 val accuracy: 94.6777777778 loss 0.00323708117008\n",
      "1 Train accuracy: 97.3333333333 val accuracy: 95.1666666667 loss 0.00226041853428\n",
      "1 Train accuracy: 96.1666666667 val accuracy: 95.4 loss 0.00119322344661\n",
      "2 Train accuracy: 96.6666666667 val accuracy: 95.6555555556 loss 0.000750882402062\n",
      "2 Train accuracy: 97.0 val accuracy: 95.6944444444 loss 0.000977539047599\n",
      "2 Train accuracy: 98.3333333333 val accuracy: 95.6861111111 loss 0.00194483324885\n",
      "2 Train accuracy: 98.0 val accuracy: 96.1388888889 loss 0.00129876792431\n",
      "2 Train accuracy: 97.5 val accuracy: 96.2916666667 loss 0.00109433218837\n",
      "3 Train accuracy: 97.6666666667 val accuracy: 96.6083333333 loss 0.000498584471643\n",
      "3 Train accuracy: 97.5 val accuracy: 96.2777777778 loss 0.000971286073327\n",
      "3 Train accuracy: 97.3333333333 val accuracy: 96.1361111111 loss 0.00228068053722\n",
      "3 Train accuracy: 98.5 val accuracy: 96.725 loss 0.00148445725441\n",
      "3 Train accuracy: 97.5 val accuracy: 96.8277777778 loss 0.00106785975397\n",
      "4 Train accuracy: 97.3333333333 val accuracy: 96.9194444444 loss 0.000507574230433\n",
      "4 Train accuracy: 98.1666666667 val accuracy: 96.5972222222 loss 0.000599515810609\n",
      "4 Train accuracy: 98.1666666667 val accuracy: 96.5277777778 loss 0.00215781912208\n",
      "4 Train accuracy: 98.1666666667 val accuracy: 96.9527777778 loss 0.00107821576297\n",
      "4 Train accuracy: 98.6666666667 val accuracy: 96.9638888889 loss 0.000441103987396\n",
      "5 Train accuracy: 98.0 val accuracy: 97.0166666667 loss 0.000391149371862\n",
      "5 Train accuracy: 99.3333333333 val accuracy: 97.0583333333 loss 0.000428630262613\n",
      "5 Train accuracy: 98.0 val accuracy: 96.9472222222 loss 0.00196598172188\n",
      "5 Train accuracy: 98.6666666667 val accuracy: 96.9833333333 loss 0.000884576737881\n",
      "5 Train accuracy: 98.5 val accuracy: 97.1333333333 loss 0.000431147776544\n",
      "acc_test 96.3957759412\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 6\n",
    "batch_size = 100\n",
    "myloss=0.0\n",
    "best_acc =0.0\n",
    "n=10\n",
    "with tf.Session(graph=mygraph1) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        if n==0:\n",
    "            break\n",
    "        for iteration in range(len(train_dataset)// batch_size):\n",
    "            if n==0:\n",
    "                break\n",
    "            offset = (iteration * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            X_batch = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            y_batch = train_labels[offset:(offset + batch_size),:]\n",
    "            sess.run([training_op], feed_dict={X: X_batch, y: y_batch, training:True})\n",
    "            \n",
    "            if iteration %500 ==0:\n",
    "                train_pre = predictions.eval(feed_dict={X:X_batch})\n",
    "                #print(type(train_pre))\n",
    "                val_pre = predictions.eval(feed_dict={X:valid_dataset})     \n",
    "                #print(type(val_pre))\n",
    "                acc_val = accuracy(val_pre, valid_labels)\n",
    "                acc_train = accuracy(train_pre,y_batch)\n",
    "                myloss = loss1.eval(feed_dict={X: X_batch, y: y_batch})/batch_size\n",
    "                print(epoch, \"Train accuracy:\", acc_train, \"val accuracy:\", acc_val,\"loss\",myloss)\n",
    "                if acc_val>best_acc:\n",
    "                    best_acc = acc_val\n",
    "                if acc_val<best_acc:\n",
    "                    n -=1\n",
    "                    if n==0:\n",
    "                        test_prediction = sess.run(predictions, feed_dict={X:test_dataset})\n",
    "                        print('acc_test',acc_test)\n",
    "                        save_path = saver.save(sess, \".ipynb_checkpoints/5.ckpt\")\n",
    "                \n",
    "        save_path = saver.save(sess, \".ipynb_checkpoints/4.ckpt\")\n",
    "    test_prediction = sess.run(predictions, feed_dict={X:test_dataset})\n",
    "    acc_test = accuracy(test_prediction,test_labels)  \n",
    "    print('acc_test',acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .ipynb_checkpoints/4.ckpt\n",
      "[[ 3  3  1  5 10 10]] [ 3  3  1  5 10 10]\n"
     ]
    }
   ],
   "source": [
    "# test use a singe example\n",
    "with tf.Session(graph=mygraph1) as sess:\n",
    "    saver.restore(sess,\".ipynb_checkpoints/4.ckpt\" )\n",
    "    pre = np.argmax(predictions.eval(feed_dict={X:train_dataset[999:1000]}),2).T\n",
    "    print(pre,train_labels[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WOW GET A BEETER RESULT THAN NUMBER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open('../dataset/svhn/test2.JPG')\n",
    "im = im.resize((32, 32),Image.ANTIALIAS )\n",
    "im1=np.array(im)\n",
    "im1=im1[np.newaxis,:,:,:]\n",
    "print(im1.shape)\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 54.8333333333 val accuracy: 53.6166666667 loss 0.0232107138634\n",
      "0 Train accuracy: 64.5 val accuracy: 64.2083333333 loss 0.0184095752239\n",
      "0 Train accuracy: 80.3333333333 val accuracy: 77.4194444444 loss 0.00959761500359\n",
      "0 Train accuracy: 90.6666666667 val accuracy: 87.675 loss 0.00435307532549\n",
      "0 Train accuracy: 92.6666666667 val accuracy: 91.3333333333 loss 0.00206595212221\n",
      "1 Train accuracy: 95.0 val accuracy: 92.6722222222 loss 0.00181042194366\n",
      "1 Train accuracy: 94.5 val accuracy: 93.3277777778 loss 0.00246467098594\n",
      "1 Train accuracy: 96.3333333333 val accuracy: 94.4916666667 loss 0.00279549896717\n",
      "1 Train accuracy: 97.0 val accuracy: 94.7555555556 loss 0.00178185865283\n",
      "1 Train accuracy: 97.3333333333 val accuracy: 95.4277777778 loss 0.000719169676304\n",
      "2 Train accuracy: 97.3333333333 val accuracy: 95.7166666667 loss 0.000640143081546\n",
      "2 Train accuracy: 96.1666666667 val accuracy: 95.5638888889 loss 0.0016784735024\n",
      "2 Train accuracy: 98.1666666667 val accuracy: 95.7166666667 loss 0.00180456578732\n",
      "2 Train accuracy: 97.1666666667 val accuracy: 96.3055555556 loss 0.0013215829432\n",
      "2 Train accuracy: 98.1666666667 val accuracy: 96.2944444444 loss 0.000302873607725\n",
      "3 Train accuracy: 97.5 val accuracy: 96.5388888889 loss 0.000397669449449\n",
      "3 Train accuracy: 97.5 val accuracy: 96.1083333333 loss 0.00149368464947\n",
      "3 Train accuracy: 97.8333333333 val accuracy: 96.4611111111 loss 0.00176226466894\n",
      "3 Train accuracy: 98.8333333333 val accuracy: 96.8833333333 loss 0.000880913734436\n",
      "3 Train accuracy: 98.0 val accuracy: 96.7333333333 loss 0.000461356304586\n",
      "4 Train accuracy: 97.5 val accuracy: 96.8416666667 loss 0.000764118432999\n",
      "4 Train accuracy: 98.5 val accuracy: 96.8805555556 loss 0.000882715433836\n",
      "4 Train accuracy: 98.5 val accuracy: 96.6305555556 loss 0.00141948193312\n",
      "4 Train accuracy: 97.6666666667 val accuracy: 96.4138888889 loss 0.00187277436256\n",
      "4 Train accuracy: 98.5 val accuracy: 97.1 loss 0.000405272059143\n",
      "5 Train accuracy: 98.0 val accuracy: 96.9638888889 loss 0.000390091501176\n",
      "acc_test 96.4212835425\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 6\n",
    "batch_size = 100\n",
    "myloss=0.0\n",
    "best_acc =0.0\n",
    "n=10\n",
    "with tf.Session(graph=mygraph1) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        if n==0:\n",
    "            break\n",
    "        for iteration in range(len(train_dataset)// batch_size):\n",
    "            if n==0:\n",
    "                break\n",
    "            offset = (iteration * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            X_batch = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            y_batch = train_labels[offset:(offset + batch_size),:]\n",
    "            sess.run([training_op], feed_dict={X: X_batch, y: y_batch, training:True})\n",
    "            \n",
    "            if iteration %500 ==0:\n",
    "                train_pre = predictions.eval(feed_dict={X:X_batch})\n",
    "                #print(type(train_pre))\n",
    "                val_pre = predictions.eval(feed_dict={X:valid_dataset})     \n",
    "                #print(type(val_pre))\n",
    "                acc_val = accuracy(val_pre, valid_labels)\n",
    "                acc_train = accuracy(train_pre,y_batch)\n",
    "                myloss = loss1.eval(feed_dict={X: X_batch, y: y_batch})/batch_size\n",
    "                print(epoch, \"Train accuracy:\", acc_train, \"val accuracy:\", acc_val,\"loss\",myloss)\n",
    "                if acc_val>best_acc:\n",
    "                    best_acc = acc_val\n",
    "                if acc_val<best_acc:\n",
    "                    n -=1\n",
    "         \n",
    "                \n",
    "        save_path = saver.save(sess, \".ipynb_checkpoints/5.ckpt\")\n",
    "    test_prediction = sess.run(predictions, feed_dict={X:test_dataset})\n",
    "    acc_test = accuracy(test_prediction,test_labels)  \n",
    "    print('acc_test',acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .ipynb_checkpoints/5.ckpt\n",
      "[[ 2  7  4 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "# test use a singe example\n",
    "with tf.Session(graph=mygraph1) as sess:\n",
    "    saver=tf.train.import_meta_graph('.ipynb_checkpoints/5.ckpt.meta')\n",
    "    saver.restore(sess,\".ipynb_checkpoints/5.ckpt\" )\n",
    "    pre = np.argmax(predictions.eval(feed_dict={X:im1}),2).T\n",
    "    print(pre)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
